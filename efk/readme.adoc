= EFK

== Introducing Fluentd

Historically, one of the most popular open source stacks for handling log records has been
the ELK stack from Elastic ( https://www.elastic.co ), based on Elasticsearch, Logstash
(used for log collection and transformation), and Kibana. Since Logstash runs on a Java VM,
it requires a relatively large amount of memory. Over the years, a number of open source
alternatives have been developed that require significantly less memory than Logstash, one
of them being Fluentd ( https://www.fluentd.org ).

Fluentd is managed by the Cloud Native Computing Foundation (CNCF) ( https://www.cncf.io ),
that is, the same organization that manages the Kubernetes project. Therefore,
Fluentd has become a natural choice as an open source-based log collector that runs in
Kubernetes. Together with Elastic and Kibana, it forms the EFK stack.

Fluentd is written in a mix of C and Ruby, using C for the performance-critical parts and
Ruby where flexibility is of more importance, for example, allowing the simple installation
of third-party plugins using Ruby's gem install command.

A log record is processed as an event in Fluentd and consists of the following information:

* A time field describing when the log record was created
* A tag field that identifies what type of log record it isâ€”the tag is used by
* Fluentd's routing engine to determine how a log record shall be processed
A record that contains the actual log information, which is stored as a JSON
object

A Fluentd configuration file is used to tell Fluentd how to collect, process, and finally send
log records to various targets, such as Elasticsearch. A configuration file consists of the
following types of core elements:

* <source> : Source elements describe where Fluentd will collect log records. For
example, tailing log files that have been written to by Docker containers. Source
elements typically tag the log records, describing the type of log record. It could,
for example, be used to tag log records to state that they come from containers
running in Kubernetes.
* <filter> : Filter elements are used to process the log records, for example, a
filter element can parse log records that come from Spring Boot-based
microservices and extract interesting parts of the log message into separate fields
in the log record. Extracting information into separate fields in the log record
makes the information searchable by Elasticsearch. A filter element selects what
log records to process based on their tags.
* <match> : Output elements are used to perform two main tasks:
Send processed log records to targets such as Elasticsearch.
Routing is to decide how to process log records. A routing rule can
rewrite the tag and reemit the log record into the Fluentd routing
engine for further processing. A routing rule is expressed as an
embedded <rule> element inside the <match> element. Output
elements decide what log records to process, in the same way as a
filter: based on the tag of the log records.

Fluentd comes with a number of built-in and external third-party plugins that are used by
the source, filter, and output elements. We will see some of them in action when we walk
through the configuration file in the next section. For more information on the available
plugins, see Fluentd's documentation, which is available at https://docs.fluentd.org .


== Configuring Fluentd

The configuration of Fluentd is based on the configuration files from a Fluentd project on
GitHub, fluentd-kubernetes-daemonset . The project contains Fluentd configuration
files for how to collect log records from containers that run in Kubernetes and how to send
them to Elasticsearch once they have been processed. We can reuse this configuration
without changes and it will simplify our own configuration to a great extent. The Fluentd
configuration files can be found at
https://github.com/fluent/fluentd-kubernetes-daemonset/tree/master/docker-image/v1.7/debian-elasticsearch7/conf.

The configuration files that provide this functionality are kubernetes.conf and
fluent.conf . The kubernetes.conf configuration file contains the following
information:

* Source elements that tail container log files and log files from processes that run
outside of Kubernetes, for example, the kubelet and the Docker daemon. The
source elements also tag the log records from Kubernetes with the full name of
the log file with / replaced by . and prefixed with kubernetes . Since the tag is
based on the full filename, the name contains the name of the namespace, pod,
and container, among other things. So, the tag is very useful for finding log
records of interest by matching the tag.
* A filter element that enriches the log records that come from containers running
inside Kubernetes, along with Kubernetes-specific fields that contain information
such as the names of the containers and the namespace they run in.

The main configuration file, fluent.conf , contains the following information:

* @include statements for other configuration files, for example,
the kubernetes.conf file we described previously. It also includes custom
configuration files that are placed in a specific folder, making it very easy for us
to reuse these configuration files without any changes and provide our own
configuration file that only handles processing related to our own log records.
We simply need to place our own configuration file in the folder specified by
the fluent.conf file.
* An output element that sends log records to Elasticsearch.

This processing is summarized by the following UML activity diagram:

image::processing-diagram.png[]

The fluentd-hands-on-configmap.yml configuration file follows this activity diagram closely.

== Deploying Elasticsearch and Kibana

For the recommended deployment in a production environment on
Kubernetes, see https://www.elastic.co/elasticsearch-kubernetes .

https://istio.io/docs/tasks/observability/logs/fluentd/

[source,]
----
eval $(minikube docker-env)
docker pull docker.elastic.co/elasticsearch/elasticsearch-oss:7.3.0
docker pull docker.elastic.co/kibana/kibana-oss:7.3.0

kubectl create namespace logging

kubectl apply -f efk/elasticsearch.yml -n logging
kubectl wait --timeout=120s --for=condition=Ready pod -n logging --all -n logging
kubectl apply -f efk/kibana.yml -n logging
kubectl wait --timeout=120s --for=condition=Ready pod -n logging --all -n logging
----

== Deploying Fluentd


[source,]
----
docker build -f efk/Dockerfile -t hands-on/fluentd:v1 efk

kubectl apply -f efk/fluentd-hands-on-configmap.yml
kubectl apply -f efk/fluentd-ds.yml
kubectl wait --timeout=120s --for=condition=Ready pod -l app=fluentd -n kube-system

# Verify is working

kubectl logs -n kube-system $(kubectl get pod -l app=fluentd -n kube-system -o jsonpath={.items..metadata.name}) | grep "fluentd worker is now running worker"
curl http://$(kubectl get -n logging service elasticsearch -o jsonpath={.spec.clusterIP}):9200/_all/_count
----

=== Trying out the EFK stack

get IP kibana
kubectl get -n logging service kibana -o jsonpath={.spec.clusterIP}

=== Initializing Kibana

. open on browser http://IP_KIBANA:5601
. On the welcome page, Welcome to Kibana, click on the Explore on my
own button.
. Click on the Expand button in the lower-left corner to view the names of the
menu choices. These will be shown on the left-hand side.
. Click on Discover in the menu to the left. You will be asked to define a pattern
that's used by Kibana to identify what Elasticsearch indices it shall retrieve log
records from.
. Enter the logstash-* index pattern and click on Next Step.
. On the next page, you will be asked to specify the name of the field that contains
the timestamp for the log records. Click on the drop-down list for the Time Filter
field name and select the only available field, @timestamp.
. Click on the Create index pattern button.
. Kibana will show a page that summarizes the fields that are available in the
selected indices.


=== Analyzing the log records

We will use Kibana's visualization feature to divide the log records per Kubernetes
namespace and then ask Kibana to show us how the log records are divided per type of
container within each namespace. A pie chart is a suitable chart type for this type of
analysis. Perform the following steps to create a pie chart:

. In Kibana's web UI, click on Visualize in the menu to the left.
. Click on the Create new visualization button.
. Select Pie as the visualization type.
. Select logstash-* as the source.
. In the time picker (a date interval selector) above the pie chart, set a date interval
of your choice (set to the last 7 days in the following screenshot). Click on its
calendar icon to adjust the time interval.
. Click on Add to create the first bucket, as follows:
.. Select the bucket type, that is, Split slices.
.. For the aggregation type, select Terms from the drop-down list.
.. As the field, select kubernetes.namespace_name.keyword.
.. For the size, select 10.
.. Enable Group other values in separate bucket.
.. Enable Show missing values.
.. Press the Apply changes button (the blue play icon above the Bucket
definition). Expect a pie chart that looks similar to the following:

image::pie-1.png[]

. Click on Add again to create a second bucket:
.. Select the bucket type, that is, Split slices.
.. As the sub-aggregation type, select Terms from the drop-down list.
.. As the field, select kubernetes.container_name.keyword.
.. For the size, select 10.
.. Enable Group other values in separate bucket.
.. Enable Show missing values.
.. Press the Apply changes button again. Expect a pie chart that looks
similar to the following:

image::pie-2.png[]

. At the top of the pie chart, we have a group of log records labeled missing , that
is, they neither have a Kubernetes namespace nor a container name specified.
What's behind these missing log records? These log records come from processes
running outside of the Kubernetes cluster in the Minikube instance and they are
stored using Syslog. They can be analyzed using Syslog-specific fields,
specifically the identifier field. Let's create a third bucket that divides log records
based on their Syslog identifier field, if any.
. Click on Add again to create a third bucket:
.. Select the bucket type, that is, Split slices.
.. As the sub-aggregation type, select Terms from the drop-down list.
.. As the field, select SYSLOG_IDENTIFIER.keyword.
.. Enable Group other values in separate bucket.
.. Enable Show missing values.
.. Press the Apply changes button and expect a pie chart that looks
similar to the following:

image::pie-3.png[]


=== Find logs with Kibana

[source,]
----
ACCESS_TOKEN=$(curl -k https://writer:secret@minikube.me:443/oauth/token -d grant_type=password -d username=dkahn -d password=password -s | jq -r .access_token)
curl -X POST -k https://minikube.me/product-composite \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $ACCESS_TOKEN" \
    --data '{"productId":1234,"name":"product name 1234","weight":1234}'
curl -H "Authorization: Bearer $ACCESS_TOKEN" -k 'https://minikube.me/product-composite/1234'
----


Perform the following steps to use the API to create log records and then use Kibana to look
up the log records:

. Get an access token with the following command:

    [source,]
    ----
    ACCESS_TOKEN=$(curl -k https://writer:secret@minikube.me:443/oauth/token -d grant_type=password -d username=dkahn -d password=password -s | jq -r .access_token)
    ----

. As mentioned in the introduction to this section we will start by creating a
product with a unique product ID. Create a minimalistic product (without
recommendations and reviews) for "productId" :1234 by executing the
following command:

    [source,]
    ----
    curl -X POST -k https://minikube.me/product-composite \
        -H "Content-Type: application/json" \
        -H "Authorization: Bearer $ACCESS_TOKEN" \
        --data '{"productId":1234,"name":"product name 1234","weight":1234}'
    ----

. Read the product with the following command:

    [source,]
    ----
    curl -H "Authorization: Bearer $ACCESS_TOKEN" -k 'https://minikube.me/product-composite/1234'
    ----

. On the Kibana web page, click on the Discover menu on the left. You will see
something like the following:

    On the Kibana web page, click on the Discover menu on the left. You will see
    something like the following:

. If you want to change the time interval, you can use the time picker. Click on its
calendar icon to adjust the time interval.
. To get a better view of the content in the log records, add some fields from the
log records to the table under the histogram. Select the fields from the list of
available fields to the left. Scroll down until the field is found. Hold the cursor
over the field and an add button will appear; click on it to add the field as a
column in the table. Select the following fields, in order:
.. spring.level, the log level
.. kubernetes.container_name, the name of the container
.. spring.trace, the trace ID used for distributed tracing
.. spring.level, the log level
    The table now contains information that is of interest regarding the log records!

. To find log records from the call to the GET API, we can ask Kibana to find log
records where the log field contains the text product.id=1234. This matches the
log output from the product composite microservice that was shown
previously. This can be done by entering log:"product.id=1234" in
the Search field and clicking on the Update button (this button can also be
labeled Refresh). Expect one log record to be found:
. Verify that the timestamp is from when you called the GET API and verify that
the name of the container that created the log record is comp, that is, verify that
the log record was sent by the product composite microservice.
. Now, we want to see the related log records from the other microservices that
participated in the process of returning information about the product with
productId 1234, that is, finding log records with the same trace ID as that of the
log record we found. To do that, place the cursor over the spring.trace field
for the log record. Two small magnifying glasses will be shown to the right of the
field, one with a + sign and one with a - sign. Click on the magnifying glass with
the + sign to filter on the trace ID.
. Clean the Search field so that the only search criteria is the filter of the trace field.
Then, click on the Update button to see the result. Expect a response similar to
the following:

    We can see a lot of detailed debug and trace messages that clutter the view; let's
    get rid of them!

. Place the cursor over a TRACE value and click on the magnifying glass with the -
sign to filter out log records with the log level set to TRACE.
. Repeat the preceding step for the DEBUG log record.
. We should now be able to see the four expected log records, one for each
microservice involved in the lookup of product information for the product with
product ID 1234:

    Also, note the filters that were applied included the trace ID but excluded log records with
    the log level set to DEBUG or TRACE.


=== Performing root cause analyses

. Run the following command to generate a fault in the product microservice
while searching for product information on the product with product ID 666 :

    curl -H "Authorization: Bearer $ACCESS_TOKEN" -k https://minikube.me/product-composite/666?faultPercent=100

    Now, we have to pretend that we have no clue about the reason for this error!
    Otherwise, the root cause analysis wouldn't be very exciting, right? Let's assume
    that we work in a support organization and have been asked to investigate some
    problems that just occurred while an end user tried to look up information
    regarding a product with product ID 666 .

. Before we start to analyze the problem, let's delete the previous search filters in
the Kibana web UI so that we can start from scratch. For each filter we defined in
the previous section, click on their close icon (an x) to remove them. After all of
the filters have been removed, the web page should look similar to the following:

. Start by selecting a time interval that includes the point in time when the
problem occurred using the time picker. For example, search the last seven days
if you know that the problem occurred within the last seven days.

. Next, search for log records with the log level set to ERROR within this
timeframe. This can be done by clicking on the spring.level field in the list
of selected fields. When you click on this field, its most commonly used values
will be displayed under it. Filter on the ERROR value by clicking on its
magnifier, shown with the + sign. Kibana will now show log records within the
selected time frame with its log level set to ERROR, like so:


. We can see a number of error messages related to product ID 666 . The top four
have the same trace ID, so this seems like a trace ID of interest to use for further
investigation.
. We can also see more error messages below the top four that seem to be related
to the same error but with different trace IDs. Those are caused by the retry
mechanism in the product composite microservice, that is, it retries the request a
couple of times before giving up and returning an error message to the caller.
. Filter on the trace ID of the first log record in the same way we did in the
previous section.

. Remove the filter of the ERROR log level to be able to see all of the records
belonging to this trace ID. Expect Kibana to respond with a lot of log records.
Look to the oldest log record, that is, the one that occurred first, that looks
suspicious. For example, it may have a WARN or ERROR log level or a strange
log message. The default sort order is showing the latest log record at the top, so
scroll down to the end and search backward (you can also change the sort order
to show the oldest log record first by clicking on the small up/down arrow next
to the Time column header). The WARN log message that says Bad luck, and
error occurred looks like it could be the root cause of the problem. Let's
investigate it further:

. Once a log record has been found that might be the root cause of the problem, it
is of great interest to be able to find the nearby stack trace describing where
exceptions were thrown in the source code. Unfortunately, the Fluentd plugin we
use for collecting multiline exceptions, fluent-plugin-detect-exceptions ,
is unable to relate stack traces to the trace ID that was used. Therefore, stack
traces will not show up in Kibana when we filter on a trace ID. Instead, we can
use a feature in Kibana for finding surrounding log records that show log records
that have occurred in near time to a specific log record.
. Expand the log record that says bad luck using the arrow to the left of the log
record. Detailed information about this specific log record will be revealed. There
is also a link named View surrounding documents; click on it to see nearby log
records. Expect a web page similar to the following:

. The log record above the bad luck log record with the stack trace for the error
message Something went wrong... looks interesting and was logged by the
product microservice just two milliseconds after it logged the bad luck log record.
They seem to be related! The stack trace in that log record points to line 90
in ProductServiceImpl.java . Looking in the source code (see ProductServiceImpl.java ), line 90 looks as follows:
throw new RuntimeException("Something went wrong...");